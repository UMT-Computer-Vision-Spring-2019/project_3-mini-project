{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Where's Waldo?\n",
    "\n",
    "Due Mar. 4th\n",
    "\n",
    "So far, we've mostly focused on using imagery to do stuff for which it is better suited than a human: calculating camera locations from imagery, finding an optimal projective transform to stitch images together, and (soon) we'll be doing \"structure from motion\" in which we create 3D models of the world from collections of 2D images.  These are tasks primarily based around measuring things and doing calculations.  On the other side of the coin is object recognition (identifying the semantic content of a scene), and the best contemporary computer vision algorithms do object recognition at roughly the level of a 2 year old human (with some exceptions).  For this (mini-)project, we're going to delve into a topic that sort of straddles the line between these two general realms of computer vision.\n",
    "\n",
    "As a motivating example, did you ever play the game Where's Waldo.  There are books filled with images like the following:\n",
    "<img src='waldo_1.jpg'>\n",
    "The objective, of course, is to find Waldo, the man in the red striped shirt and beanie wearing glasses.  He looks like this:\n",
    "<img src='waldo_template.jpg'>\n",
    "These scenes are (obviously) intended to have a bunch of visual clutter to make this task reasonably challenging.\n",
    "\n",
    "Your task will be to come up with an algorithm that locates the template image (Waldo's face) and the target image (the larger scene).  This is called *template matching*, and it's a primitive form of feature recognition.  \n",
    "\n",
    "## Implementation\n",
    "### Template Matching\n",
    "Template matching works in a way that is very similar to filtering:  slide the template image over every location in the target image, computing some sort of metric at each position.  In practice, one commonly used choice for an error metric is the one that you've already used for matching keypoint descriptors: z-normalized sum square error.  Another choice is [normalized cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation#Normalization).  Once these metrics have been computed, simply find the argmin (for SSE) or argmax (for NCC), and this will be the location of the best match.  \n",
    "\n",
    "**Your task is to implement template matching.  Use 'waldo_template.jpg' as the template and 'waldo_1.jpg' as the target image.  Where's Waldo? **\n",
    "\n",
    "### Not so fast!!!  What about scale!\n",
    "Oh, no.  As it turns out, the template I've provided is not the same scale as the Waldo in the image.  To deal with this, you'll need to create an image pyramid for the template (See Szeliski 3.5, and [Mubarak Shah's lecture on this topic](https://www.youtube.com/watch?v=KO7jJt0WHag&feature=youtu.be)).  This essentially just means creating a sequence of downsampled images of the template, and trying each one in hopes that one of the resulting down-scaled templates matches the feature in the target image.  **Create a sequence of templates with which to perform feature matching, each one 1/2 the resolution of the previous (so 1/4 the total number of pixels).  To avoid aliasing, before downsampling perform a $\\sigma=1$ Gaussian Blur of the image.  Once you've built your image pyramid, find the argmin/max in 3 dimensions (u,v,template scale)**.\n",
    "\n",
    "## Generalization\n",
    "**Waldo appears in every Where's Waldo image (obviously).  Try using the same technique on 'waldo_2.jpg'.  Does the algorithm work?**  I confess that I pulled the image of waldo for the template directly from 'waldo_1.jpg', so for the correct scale, there is something close to an exact match (i.e. SSE=0).  However, Waldo, while easily recognizable to the human eye after undergoing the small scale deformations associated with artistic license, is not so easily recognizable via template matching.  We will return to a similar problem when discussing object recognition, and hopefully this example will motivate the need to come up with representations of objects (like Waldo) that are more robust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import matplotlib.patches as patches\n",
    "import math as mt\n",
    "import time\n",
    "\n",
    "\n",
    "class templateMatch(object):\n",
    "\n",
    "    def __init__(self,baseImage,template,downsample):\n",
    "        self.baseImage = plt.imread(baseImage).mean(axis=2)\n",
    "        self.template = plt.imread(template).mean(axis=2)\n",
    "        self.downsample = downsample\n",
    "        self.matches = []\n",
    "\n",
    "    \n",
    "    def gauss_kernal(self,size, var):\n",
    "        kernel = np.zeros(shape=(size,size))\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                kernel[i][j] = mt.exp( -((i - (size-1)/2)**2 ))\n",
    "        kernel = kernel / kernel.sum()\n",
    "        return kernel\n",
    "                                     \n",
    "    def convolve(self,g,h): # h is kernel, g is the image\n",
    "        \n",
    "        I_gray_copy = np.zeros(shape=(len(g[:,1]),len(g[1,:])))\n",
    "        u,v = h.shape\n",
    "        sp = int(u/2)\n",
    "\n",
    "        start = sp\n",
    "        for i in range(start,len(g[:,1])-1):\n",
    "            for j in range(start, len(g[i,:])-1):\n",
    "                f = g[i-sp:i+sp, j-sp:j+sp] #FIXME\n",
    "                # need another array to put value into\n",
    "                total = h*f\n",
    "                I_gray_copy[i][j] = sum(sum(total))\n",
    "        return I_gray_copy\n",
    "\n",
    "    def downSample(self,image,n=None):\n",
    "        if n is None:\n",
    "            n = int(self.downsample)\n",
    "        pyramid = []\n",
    "        for i in range(n):\n",
    "            image_conv = self.convolve(image,self.gauss_kernal(4,1))\n",
    "            pyramid.append(image_conv)\n",
    "            image = image[::2,::2]\n",
    "        return pyramid\n",
    "\n",
    "    def templMatch(self,BaseImage,matchImage):\n",
    "        TA = time.time()\n",
    "        U,V = BaseImage.shape\n",
    "        pyramid = self.downSample(matchImage)\n",
    "        #count = 0 #FIXME\n",
    "        #print(\"U,V : \",U,V)\n",
    "        for i in range(len(pyramid)):\n",
    "            subMatchImage = pyramid[i]\n",
    "            mU,mV = subMatchImage.shape\n",
    "            argmin = np.inf\n",
    "            bestU,bestV = None,None\n",
    "            #print(\"Cross corr test start\\n\")\n",
    "            for j in range(0,U-mU):\n",
    "                for k in range(0,V-mV):\n",
    "                \n",
    "                   # compare = self.SSE(BaseImage[j:mU+j,k:mV+k],matchImage)\n",
    "                    compare = ((BaseImage[j:mU+j,k:mV+k] - subMatchImage)**2).sum()/(mU*mV)\n",
    "                \n",
    "                    if(compare < argmin):\n",
    "                       #count += 1\n",
    "                       #print(\"Better match found,count: \",count)\n",
    "                       argmin = compare\n",
    "                       bestU = j\n",
    "                       bestV = k\n",
    "                #print(\"j loop: \",j)\n",
    "            self.matches.append((i+1,bestU,bestV,argmin))\n",
    "            #print(\"Cross corr test end\\n\")\n",
    "        self.matches.sort(key = lambda x:x[-1])\n",
    "        print(\"Best location estimate: \", self.matches[0][1],self.matches[0][2])\n",
    "        print(\"All estimates: \\n\")\n",
    "\n",
    "        for i in range(int(self.downsample)):\n",
    "            print(self.matches[i][0] ,\" \" ,self.matches[i][1], \" \", self.matches[i][2],\" \",self.matches[i][3])\n",
    "\n",
    "        print(\"Elapsed Time: \",time.time()-TA, \"\\n\")              \n",
    "        return self.matches\n",
    "\n",
    "    def plotBestMatch(self,matches,baseImage,template):\n",
    "        lev = matches[0][0]\n",
    "        tx,ty = template.shape\n",
    "        tx,ty = tx/lev, ty/lev\n",
    "        bx,by = baseImage.shape\n",
    "        X,Y = lev*matches[0][1], lev*matches[0][2] #location of best match in base image\n",
    "        #coords for square\n",
    "        cX,cY = X+(tx/2),Y+(ty/2) #center of square\n",
    "        #p1,p2,p3,p4 = [X,Y], [X + tx/2,Y], [X + tx/2,Y + ty/2], [X ,Y + ty/2]\n",
    "        fig,ax = plt.subplots(1)\n",
    "        ax.imshow(self.baseImage)\n",
    "        rect = patches.Rectangle([self.matches[0][1],self.matches[0][2]],ty,tx,linewidth=5,edgecolor='g',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        plt.show()\n",
    "        #plt.savefig(\"WheresWaldo.jpg\")\n",
    "\n",
    "    def runAndPlot(self):\n",
    "        match = self.templMatch(self.baseImage,self.template)\n",
    "        self.plotBestMatch(match,self.baseImage,self.template)\n",
    "\n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WheresWaldo = templateMatch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
